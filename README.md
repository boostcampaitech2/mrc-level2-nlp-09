# KLUE Open-Domain Question Answering, Naver Boostcamp AI Tech 2ê¸°
## Competition Abstract
ğŸ¤“ KLUE MRC(Machine Reading Comprehension) Datasetìœ¼ë¡œ Open-Domain Question Answeringì„ ìˆ˜í–‰í•˜ëŠ” Task.  
ğŸ¤“ ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„ì£¼ëŠ” Retrieverì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì½ê³  ë‹µë³€ì„ í•˜ëŠ” Readerë¡œ êµ¬ì„±.  
ğŸ¤“ Leaderboardì—ì„œ Public 240ê°œ, Private 360ê°œë¡œ í‰ê°€ê°€ ì´ë£¨ì–´ì§.  
ğŸ¤“ í•˜ë£¨ 10íšŒë¡œ ëª¨ë¸ ì œì¶œ ì œí•œ

## [Team Portfolio](/)
## [Competition Report(PDF)](/)
## Our solutions
- Retreiver
  - Elastic search
  - Pororo NER
- Augmentation
  - Negative Sampling
  - Question Generation
- Post Processing
  - Top-k Passages Seperate
  - Answer scroing with softmax
  - Similiarity scoring with KSS(Korean Sentence Spliter)
  - Other post-processing via Mecab
- Ensemble
  - Hard voting
  - Post processing 

## ìµœì¢… ìˆœìœ„ 2ë“±!
<img src="competition_results/capture.png" width="80%">

--- 
## Docs 


## Quickstart
### Installation
```
pip install -r requirements.txt
```
### Train model
```python
# default wandb setting in train.py
run = wandb.init(project= 'klue', entity= 'quarter100', name= f'Any training name')
```

```
python train.py
```
Models are saved in "./models/train_dataset_{experiment_name}/".
### Inference
```
python inference.py --output_dir ./outputs/test_dataset/ --dataset_name ../data/test_dataset/ --model_name_or_path ./models/train_dataset/ --do_predict
```
Prediction csv files are saved in "./outputs/test_dataset/".
### Ensemble
Check hard_voting.ipynb.  
Ensemble result is saved in "./submission_fold_total.csv".

## Members

[ê¹€ë‹¤ì˜](https://github.com/keemdy), [ê¹€ë‹¤ì¸](https://github.com/danny980521), [ë°•ì„±í˜¸](https://github.com/naem1023), [ë°•ì¬í˜•](https://github.com/Jay-Ppark), [ì„œë™ê±´](https://github.com/donggunseo), [ì •ë¯¼ì§€](https://github.com/minji-o-j), [ìµœì„ë¯¼](https://github.com/RockMiin)

## Advisors
[ë°•ì±„í›ˆ ë©˜í† ë‹˜](https://github.com/ddehun)