
## Training  
1. feat/exp/kfold pull ë°›ê¸°  
2. make_ner_tag.py ëŒë ¤ì„œ train_tagged.csv, inference_tagged.csv ìƒì„±
3. make_folds.py ëŒë ¤ì„œ fold1.csv ~  fold5.csv ìƒì„±
4. elastic_setting.py ëŒë¦¬ê¸° (../data/wikipedia_documents.json) ì´ í˜•ì‹ ë§ì¶°ë†“ê¸°  
5. elastic_retriever.py ëŒë¦´í•„ìš”ëŠ” ì—†ëŠ”ë° ì•ˆì— íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìˆ˜!!  
	inference_tagged.csv (code ë””ë ‰ ì•ˆì—)  
	train_tagged.csv (code ë””ë ‰ ì•ˆì—)  
6. trian.py ëŒë¦¬ë©´ ë˜ëŠ”ë° ìˆì–´ì•¼ ë˜ëŠ” íŒŒì¼ë“¤  
	qg_dataset ë””ë ‰ ë§Œë“¤ê¸° (dataë””ë ‰ì•ˆì— ë§Œë“¤ê¸°)  
	fold1.csv ~  fold5.csv (code ë””ë ‰ ì•ˆì—)  
	delete_qg_sort.pkl (data ë””ë ‰ ì•ˆì—)  
	train_tagged.csv (code ë””ë ‰ ì•ˆì—) 


## Inference
<<python inference.py --output_dir ./outputs/test_dataset/ --dataset_name ../data/test_dataset/ --model_name_or_path ./models/train_dataset/ --do_predict>>
ì˜ í˜•ì‹ìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. --model_name_or_pathëŠ” ì €ì¥ëœ ê°€ì¥ ì¢‹ì€ íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” í´ë”ë¥¼ ë„£ì–´ì£¼ë©´ ë¨

## ì£¼ì˜ì‚¬í•­
train.py 125ë²ˆì¤„ wandb ì´ë¦„ ë³¸ì¸ ì‹¤í—˜ì— ë§ê²Œ ë°”ê¿”ì„œ ëŒë¦´ê²ƒ \\
íŒŒì¼ ê²½ë¡œ ë‹¤ ìƒëŒ€ê²½ë¡œë¡œ ë°”ê¾¸ê¸´ í–‡ëŠ”ë° í˜¹ì‹œ ëª¨ë¥´ë‹ˆ í•œ ë²ˆ ë” í™•ì¸ ë°”ëŒ \\
ì œì‹œí•œ ë³€ìˆ˜ì´ì™¸ì— ë‹¤ë¥¸ ê±° ë§Œì§€ì§€ ë§ê²ƒ \\
ì—ëŸ¬ê°€ ëœ¨ë©´ ì„œë™ê±´ì—ê²Œ ë§í•´ì£¼ì„¸ìš” \\

# KLUE Open-Domain Question Answering, Naver Boostcamp AI Tech 2ê¸°
## Competition Abstract
ğŸ¤“ KLUE MRC(Machine Reading Comprehension) Datasetìœ¼ë¡œ Open-Domain Question Answeringì„ ìˆ˜í–‰í•˜ëŠ” Task.  
ğŸ¤“ ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„ì£¼ëŠ” Retrieverì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì½ê³  ë‹µë³€ì„ í•˜ëŠ” Readerë¡œ êµ¬ì„±.  
ğŸ¤“ Leaderboardì—ì„œ Public 240ê°œ, Private 360ê°œë¡œ í‰ê°€ê°€ ì´ë£¨ì–´ì§.  
ğŸ¤“ í•˜ë£¨ 10íšŒë¡œ ëª¨ë¸ ì œì¶œ ì œí•œ

## [Team Portfolio](/)
## [Competition Report(PDF)](/)
## Our solutions
- Retreiver
  - Elastic search
  - Pororo NER
- Augmentation
  - Negative Sampling
  - Question Generation
- Post Processing
  - Top-k Passages Seperate
  - Answer scroing with softmax
  - Similiarity scoring with KSS(Korean Sentence Spliter)
  - Other post-processing via Mecab
- Ensemble
  - Hard voting
  - Post processing 

## ìµœì¢… ìˆœìœ„ 2ë“±!
<img src="competition_results/capture.png" width="80%">

--- 
## Docs 


## Quickstart
### Installation
```
pip install -r requirements.txt
```
### Train model
```python
# default wandb setting in train.py
run = wandb.init(project= 'klue', entity= 'quarter100', name= f'Any training name')
```

```
python train.py
```
Models are saved in "./models/train_dataset_{experiment_name}/".
### Inference
```
python inference.py --output_dir ./outputs/test_dataset/ --dataset_name ../data/test_dataset/ --model_name_or_path ./models/train_dataset/ --do_predict
```
Prediction csv files are saved in "./outputs/test_dataset/".
### Ensemble
Check hard_voting.ipynb.  
Ensemble result is saved in "./submission_fold_total.csv".

## Members

[ê¹€ë‹¤ì˜](https://github.com/keemdy), [ê¹€ë‹¤ì¸](https://github.com/danny980521), [ë°•ì„±í˜¸](https://github.com/naem1023), [ë°•ì¬í˜•](https://github.com/Jay-Ppark), [ì„œë™ê±´](https://github.com/donggunseo), [ì •ë¯¼ì§€](https://github.com/minji-o-j), [ìµœì„ë¯¼](https://github.com/RockMiin)

## Advisors
[ë°•ì±„í›ˆ ë©˜í† ë‹˜](https://github.com/ddehun)
